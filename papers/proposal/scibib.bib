% scibib.bib

% Disease papers

@article{coultas1994epidemiology,
  title={The epidemiology of interstitial lung diseases.},
  author={Coultas, David B and Zumwalt, Ross E and Black, William C and Sobonya, Richard E},
  journal={American journal of respiratory and critical care medicine},
  volume={150},
  number={4},
  pages={967--972},
  year={1994},
  publisher={American Public Health Association},
  doi={10.1164/ajrccm.150.4.7921471}
}

@article{daniil2008serum,
  title={Serum levels of oxidative stress as a marker of disease severity in idiopathic pulmonary fibrosis},
  author={Daniil, Zoe D and Papageorgiou, Evangelia and Koutsokera, Agela and Kostikas, Konstantinos and Kiropoulos, Theodoros and Papaioannou, Andriana I and Gourgoulianis, Konstantinos I},
  journal={Pulmonary pharmacology \& therapeutics},
  volume={21},
  number={1},
  pages={26--31},
  year={2008},
  publisher={Elsevier},
  doi={10.1016/j.pupt.2006.10.005}
}

@article{du20146,
  title={6-Minute walk distance is an independent predictor of mortality in patients with idiopathic pulmonary fibrosis},
  author={du Bois, Roland M and Albera, Carlo and Bradford, Williamson Z and Costabel, Ulrich and Leff, Jonathan A and Noble, Paul W and Sahn, Steven A and Valeyre, Dominique and Weycker, Derek and King, Talmadge E},
  journal={European Respiratory Journal},
  volume={43},
  number={5},
  pages={1421--1429},
  year={2014},
  publisher={Eur Respiratory Soc},
  doi={10.1183/09031936.00131813}
}

@article{gross2001idiopathic,
  title={Idiopathic pulmonary fibrosis},
  author={Gross, Thomas J and Hunninghake, Gary W},
  journal={New England Journal of Medicine},
  volume={345},
  number={7},
  pages={517--525},
  year={2001},
  publisher={Mass Medical Soc},
  doi={10.1056/NEJMra003200}
}

@article{king2011idiopathic,
    title={Idiopathic pulmonary fibrosis},
    author={King Jr, Talmadge E and Pardo, Annie and Selman, Mois{\'e}s},
    journal={The Lancet},
    volume={378},
    number={9807},
    pages={1949--1961},
    year={2011},
    publisher={Elsevier},
    doi={10.1016/S0140-6736(11)60052-4}
}

@article{king2014phase,
  title={A phase 3 trial of pirfenidone in patients with idiopathic pulmonary fibrosis},
  author={King Jr, Talmadge E and Bradford, Williamson Z and Castro-Bernardini, Socorro and Fagan, Elizabeth A and Glaspole, Ian and Glassberg, Marilyn K and Gorina, Eduard and Hopkins, Peter M and Kardatzke, David and Lancaster, Lisa and others},
  journal={New England Journal of Medicine},
  volume={370},
  number={22},
  pages={2083--2092},
  year={2014},
  publisher={Mass Medical Soc},
  doi={10.1056/NEJMoa1402582}
}

@article{maldonado2014automated,
  title={Automated quantification of radiological patterns predicts survival in idiopathic pulmonary fibrosis},
  author={Maldonado, Fabien and Moua, Teng and Rajagopalan, Srinivasan and Karwoski, Ronald A and Raghunath, Sushravya and Decker, Paul A and Hartman, Thomas E and Bartholmai, Brian J and Robb, Richard A and Ryu, Jay H},
  journal={European Respiratory Journal},
  volume={43},
  number={1},
  pages={204--212},
  year={2014},
  publisher={Eur Respiratory Soc},
  doi={10.1183/09031936.00071812}
}

@article{mason1999pharmacological,
    title={Pharmacological therapy for idiopathic pulmonary fibrosis: past, present, and future},
    author={Mason, Robert J and Schwarz, Marvin I and Hunninghake, Gary W and Musson, Robert A},
    journal={American Journal of Respiratory and Critical Care Medicine},
    volume={160},
    number={5},
    pages={1771--1777},
    year={1999},
    publisher={American Thoracic Society New York, NY},
    doi={10.1164/ajrccm.160.5.9903009}
}

@article{papiris2005tauhe,
  title={The Medical Research Council dyspnea scale in the estimation of disease severity in idiopathic pulmonary fibrosis},
  author={Papiris, Spyros A and Daniil, Zoe D and Malagari, Katerina and Kapotsis, Giorgos E and Sotiropoulou, Christina and Milic-Emili, Joseph and Roussos, Charis},
  journal={Respiratory medicine},
  volume={99},
  number={6},
  pages={755--761},
  year={2005},
  publisher={Elsevier},
  doi={10.1016/j.rmed.2004.10.018}
}

@article{raghu2015official,
  title={An official ATS/ERS/JRS/ALAT clinical practice guideline: treatment of idiopathic pulmonary fibrosis. An update of the 2011 clinical practice guideline},
  author={Raghu, Ganesh and Rochwerg, Bram and Zhang, Yuan and Garcia, Carlos A Cuello and Azuma, Arata and Behr, Juergen and Brozek, Jan L and Collard, Harold R and Cunningham, William and Homma, Sakae and others},
  journal={American journal of respiratory and critical care medicine},
  volume={192},
  number={2},
  pages={e3--e19},
  year={2015},
  publisher={American Thoracic Society},
  doi={10.1164/rccm.201506-1063ST}
}

@article{raghu2018diagnosis,
  title={Diagnosis of idiopathic pulmonary fibrosis. An official ATS/ERS/JRS/ALAT clinical practice guideline},
  author={Raghu, Ganesh and Remy-Jardin, Martine and Myers, Jeffrey L and Richeldi, Luca and Ryerson, Christopher J and Lederer, David J and Behr, Juergen and Cottin, Vincent and Danoff, Sonye K and Morell, Ferran and others},
  journal={American journal of respiratory and critical care medicine},
  volume={198},
  number={5},
  pages={e44--e68},
  year={2018},
  publisher={American Thoracic Society},
  doi={10.1164/rccm.201807-1255ST}
}

@article{robbie2017evaluating,
  title={Evaluating disease severity in idiopathic pulmonary fibrosis},
  author={Robbie, Hasti and Daccord, C{\'e}cile and Chua, Felix and Devaraj, Anand},
  journal={European Respiratory Review},
  volume={26},
  number={145},
  year={2017},
  publisher={Eur Respiratory Soc},
  doi={10.1183/16000617.0051-2017}
}

@article{travis2013official,
  title={An official American Thoracic Society/European Respiratory Society statement: update of the international multidisciplinary classification of the idiopathic interstitial pneumonias},
  author={Travis, William D and Costabel, Ulrich and Hansell, David M and King Jr, Talmadge E and Lynch, David A and Nicholson, Andrew G and Ryerson, Christopher J and Ryu, Jay H and Selman, Mois{\'e}s and Wells, Athol U and others},
  journal={American journal of respiratory and critical care medicine},
  volume={188},
  number={6},
  pages={733--748},
  year={2013},
  publisher={American Thoracic Society},
  doi={10.1164/rccm.201308-1483ST}
}

@article{van2016cough,
  title={Cough in idiopathic pulmonary fibrosis},
  author={van Manen, Mirjam JG and Birring, Surinder S and Vancheri, Carlo and Cottin, Vincent and Renzoni, Elisabetta A and Russell, Anne-Marie and Wijsenbeek, Marlies S},
  journal={European Respiratory Review},
  volume={25},
  number={141},
  pages={278--286},
  year={2016},
  publisher={Eur Respiratory Soc},
  doi={10.1183/16000617.0090-2015}
}

@article{walsh2018deep,
  title={Deep learning for classifying fibrotic lung disease on high-resolution computed tomography: a case-cohort study},
  author={Walsh, Simon LF and Calandriello, Lucio and Silva, Mario and Sverzellati, Nicola},
  journal={The Lancet Respiratory Medicine},
  volume={6},
  number={11},
  pages={837--845},
  year={2018},
  publisher={Elsevier},
  doi={10.1016/S2213-2600(18)30286-8}
}


% Machine Learning Papers

@article{cruz2006applications,
  title={Applications of machine learning in cancer prediction and prognosis},
  author={Cruz, Joseph A and Wishart, David S},
  journal={Cancer informatics},
  volume={2},
  pages={117693510600200030},
  year={2006},
  publisher={SAGE Publications Sage UK: London, England}, 
  doi={10.1177/117693510600200030}
}

@article{dietterich2002ensemble,
  title={Ensemble learning},
  author={Dietterich, Thomas G and others},
  journal={The handbook of brain theory and neural networks},
  volume={2},
  pages={110--125},
  year={2002},
  publisher={MIT press Cambridge, Massachusetts}
}

@book{friedman2001elements,
  title={The elements of statistical learning},
  author={Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
  volume={1},
  number={10},
  year={2001},
  publisher={Springer series in statistics New York}
}

@misc{jia2020deep,
      title={Deep Learning for Quantile Regression: DeepQuantreg}, 
      author={Yichen Jia and Jong-Hyeon Jeong},
      year={2020},
      eprint={2007.07056},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{kukar1996machine,
  title={Machine learning in prognosis of the femoral neck fracture recovery},
  author={Kukar, Matja{\v{z}} and Kononenko, Igor and Silvester, Toma},
  journal={Artificial intelligence in medicine},
  volume={8},
  number={5},
  pages={431--451},
  year={1996},
  publisher={Elsevier},
  doi={10.1016/S0933-3657(96)00351-X}
}

@inproceedings{mwebaze2016machine,
  title={Machine learning for plant disease incidence and severity measurements from leaf images},
  author={Mwebaze, Ernest and Owomugisha, Godliver},
  booktitle={2016 15th IEEE international conference on machine learning and applications (ICMLA)},
  pages={158--163},
  year={2016},
  organization={IEEE},
  doi={10.1109/ICMLA.2016.0034}
}

@article{netzer2011reading,
  title={Reading digits in natural images with unsupervised feature learning},
  author={Netzer, Yuval and Wang, Tao and Coates, Adam and Bissacco, Alessandro and Wu, Bo and Ng, Andrew Y},
  year={2011}
}

@misc{tensorflow2015-whitepaper,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{\i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@article{walsh2018role,
  title={Role of imaging in progressive-fibrosing interstitial lung diseases},
  author={Walsh, Simon LF and Devaraj, Anand and Enghelmayer, Juan Ignacio and Kishi, Kazuma and Silva, Rafael S and Patel, Nina and Rossman, Milton D and Valenzuela, Claudia and Vancheri, Carlo},
  journal={European Respiratory Review},
  volume={27},
  number={150},
  year={2018},
  publisher={Eur Respiratory Soc},
  doi={10.1183/16000617.0073-2018}
}

@article{wang2010high,
  title={High-dimensional pattern regression using machine learning: from medical images to continuous clinical variables},
  author={Wang, Ying and Fan, Yong and Bhatt, Priyanka and Davatzikos, Christos},
  journal={Neuroimage},
  volume={50},
  number={4},
  pages={1519--1535},
  year={2010},
  publisher={Elsevier},
  doi={10.1016/j.neuroimage.2009.12.092}
}

@inproceedings{xu2014deep,
    title={Deep convolutional neural network for image deconvolution},
    author={Xu, Li and Ren, Jimmy SJ and Liu, Ce and Jia, Jiaya},
    booktitle={Advances in neural information processing systems},
    pages={1790--1798},
    year={2014}
}

@inproceedings{10.1145/2939672.2939785,
author = {Chen, Tianqi and Guestrin, Carlos},
title = {XGBoost: A Scalable Tree Boosting System},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2939785},
doi = {10.1145/2939672.2939785},
abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {785–794},
numpages = {10},
keywords = {large-scale machine learning},
location = {San Francisco, California, USA},
series = {KDD '16}
}

@book{10.5555/1593511,
 author = {Van Rossum, Guido and Drake, Fred L.},
 title = {Python 3 Reference Manual},
 year = {2009},
 isbn = {1441412697},
 publisher = {CreateSpace},
 address = {Scotts Valley, CA}
} 

@inproceedings{10.5555/2986916.2987033,
author = {Krogh, Anders and Hertz, John A.},
title = {A Simple Weight Decay Can Improve Generalization},
year = {1991},
isbn = {1558602224},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {It has been observed in numerical simulations that a weight decay can improve generalization in a feed-forward neural network. This paper explains why. It is proven that a weight decay has two effects in a linear network. First, it suppresses any irrelevant components of the weight vector by choosing the smallest vector that solves the learning problem. Second, if the size is chosen right, a weight decay can suppress some of the effects of static noise on the targets, which improves generalization quite a lot. It is then shown how to extend these results to networks with hidden layers and non-linear units. Finally the theory is confirmed by some numerical simulations using the data from NetTalk.},
booktitle = {Proceedings of the 4th International Conference on Neural Information Processing Systems},
pages = {950–957},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'91}
}

@inproceedings{10.5555/3045796.3045801,
author = {Baldi, Pierre},
title = {Autoencoders, Unsupervised Learning and Deep Architectures},
year = {2011},
publisher = {JMLR.org},
abstract = {Autoencoders play a fundamental role in unsupervised learning and in deep architectures for transfer learning and other tasks. In spite of their fundamental role, only linear autoencoders over the real numbers have been solved analytically. Here we present a general mathematical framework for the study of both linear and non-linear autoencoders. The framework allows one to derive an analytical treatment for the most non-linear autoencoder, the Boolean autoencoder. Learning in the Boolean autoencoder is equivalent to a clustering problem that can be solved in polynomial time when the number of clusters is small and becomes NP complete when the number of clusters is large. The framework sheds light on the different kinds of autoencoders, their learning complexity, their horizontal and vertical composability in deep architectures, their critical points, and their fundamental connections to clustering, Hebbian learning, and information theory.},
booktitle = {Proceedings of the 2011 International Conference on Unsupervised and Transfer Learning Workshop - Volume 27},
pages = {37–50},
numpages = {14},
keywords = {clustering, hebbian learning, boolean, deep architectures, principal component analysis, compression, information theory, complexity, unsupervised learning, autoencoders},
location = {Washington, USA},
series = {UTLW'11}
}

@book{10.5555/3239815,
author = {Zheng, Alice and Casari, Amanda},
title = {Feature Engineering for Machine Learning: Principles and Techniques for Data Scientists},
year = {2018},
isbn = {1491953241},
publisher = {O'Reilly Media, Inc.},
edition = {1st},
abstract = {Feature engineering is a crucial step in the machine-learning pipeline, yet this topic is rarely examined on its own. With this practical book, youll learn techniques for extracting and transforming featuresthe numeric representations of raw datainto formats for machine-learning models. Each chapter guides you through a single data problem, such as how to represent text or image data. Together, these examples illustrate the main principles of feature engineering. Rather than simply teach these principles, authors Alice Zheng and Amanda Casari focus on practical application with exercises throughout the book. The closing chapter brings everything together by tackling a real-world, structured dataset with several feature-engineering techniques. Python packages including numpy, Pandas, Scikit-learn, and Matplotlib are used in code examples. Youll examine: Feature engineering for numeric data: filtering, binning, scaling, log transforms, and power transforms Natural text techniques: bag-of-words, n-grams, and phrase detection Frequency-based filtering and feature scaling for eliminating uninformative features Encoding techniques of categorical variables, including feature hashing and bin-counting Model-based feature engineering with principal component analysis The concept of model stacking, using k-means as a featurization technique Image feature extraction with manual and deep-learning techniques}
}

@book{10.5555/525544,
author = {Neal, Radford M.},
title = {Bayesian Learning for Neural Networks},
year = {1996},
isbn = {0387947248},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {From the Publisher:Artificial "neural networks" are now widely used as flexible models for regression classification applications, but questions remain regarding what these models mean, and how they can safely be used when training data is limited. Bayesian Learning for Neural Networks shows that Bayesian methods allow complex neural network models to be used without fear of the "overfitting" that can occur with traditional neural network learning methods. Insight into the nature of these complex Bayesian models is provided by a theoretical investigation of the priors over functions that underlie them. Use of these models in practice is made possible using Markov chain Monte Carlo techniques. Both the theoretical and computational aspects of this work are of wider statistical interest, as they contribute to a better understanding of how Bayesian methods can be applied to complex problems. Presupposing only the basic knowledge of probability and statistics, this book should be of interest to many researchers in statistics, engineering, and artificial intelligence. Software for Unix systems that implements the methods described is freely available over the Internet.}
}

@book{10.5555/553011,
author = {Jain, L. C. and Medsker, L. R.},
title = {Recurrent Neural Networks: Design and Applications},
year = {1999},
isbn = {0849371813},
publisher = {CRC Press, Inc.},
address = {USA},
edition = {1st},
abstract = {From the Publisher:With applications ranging from motion detection to financial forecasting, recurrent neural networks (RNNs) have emerged as an interesting and important part of neural network research. Recurrent Neural Networks: Design and Applications reflects the tremendous, worldwide interest in and virtually unlimited potential of RNNs - providing a summary of the design, applications, current research, and challenges of this dynamic and promising field.}
}

@inproceedings{10.5555/648054.743935,
author = {Dietterich, Thomas G.},
title = {Ensemble Methods in Machine Learning},
year = {2000},
isbn = {3540677046},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Ensemble methods are learning algorithms that construct a set of classifiers and then classify new data points by taking a (weighted) vote of their predictions. The original ensemble method is Bayesian averaging, but more recent algorithms include error-correcting output coding, Bagging, and boosting. This paper reviews these methods and explains why ensembles can often perform better than any single classifier. Some previous studies comparing ensemble methods are reviewed, and some new experiments are presented to uncover the reasons that Adaboost does not overfit rapidly.},
booktitle = {Proceedings of the First International Workshop on Multiple Classifier Systems},
pages = {1–15},
numpages = {15},
series = {MCS '00}
}

% Kaggle Data Source

@misc{kaggle,
  author={Open Source Imaging Consortium},
  title={OSIC Pulmonary Fibrosis Progression},
  url={https://www.kaggle.com/c/osic-pulmonary-fibrosis-progression},
  year={2020},
  publisher={Kaggle},
  urldate={25/10/20},
}